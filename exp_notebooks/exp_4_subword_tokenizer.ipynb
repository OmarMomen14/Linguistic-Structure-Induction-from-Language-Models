{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.treebank import TreebankWordTokenizer, TreebankWordDetokenizer\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "#from structformer import StructFormer\n",
    "import data_penn\n",
    "import data_ptb\n",
    "import numpy as np\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tree_utils\n",
    "import collections\n",
    "from nltk.parse import DependencyGraph\n",
    "import nltk\n",
    "from torch.nn import functional as F\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import data_ptb_subword\n",
    "import re\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_TAGS = [\n",
    "    'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN',\n",
    "    'NNS', 'NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP',\n",
    "    'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP',\n",
    "    'WP$', 'WRB'\n",
    "]\n",
    "\n",
    "detok = TreebankWordDetokenizer()\n",
    "\n",
    "#ptb_corpus = data_ptb.Corpus('../data/penn/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_words(tree):\n",
    "    words = []\n",
    "    for w, tag in tree.pos():\n",
    "      if tag in WORD_TAGS:\n",
    "        w = w.lower()\n",
    "        words.append(w)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ptb_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fd4f1564561e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdetok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mptb_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_nltktrees\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ptb_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "detok.detokenize(filter_words(ptb_corpus.train_nltktrees[-3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_sens = []\n",
    "for tree in ptb_corpus.train_nltktrees:\n",
    "    sen = detok.detokenize(filter_words(tree))\n",
    "    all_train_sens.append(sen)\n",
    "    \n",
    "all_test_sens = []\n",
    "for tree in ptb_corpus.test_nltktrees:\n",
    "    sen = detok.detokenize(filter_words(tree))\n",
    "    all_test_sens.append(sen)\n",
    "    \n",
    "all_val_sens = []\n",
    "for tree in ptb_corpus.valid_nltktress:\n",
    "    sen = detok.detokenize(filter_words(tree))\n",
    "    all_val_sens.append(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41825, 2416, 1700)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_train_sens), len(all_test_sens), len(all_val_sens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/penn_subword/train_filtered_detok.txt', 'w') as f:\n",
    "    for sen in all_train_sens:\n",
    "        f.write(sen + '\\n')\n",
    "\n",
    "with open('../data/penn_subword/test_filtered_detok.txt', 'w') as f:\n",
    "    for sen in all_test_sens:\n",
    "        f.write(sen + '\\n')\n",
    "        \n",
    "with open('../data/penn_subword/val_filtered_detok.txt', 'w') as f:\n",
    "    for sen in all_val_sens:\n",
    "        f.write(sen + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tokenizer = transformers.AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus():\n",
    "    global c\n",
    "    c = 0\n",
    "    with open('../data/penn_subword/train_filtered_detok.txt') as f:\n",
    "        for line in f:\n",
    "            c += 1\n",
    "            yield line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer = old_tokenizer.train_new_from_iterator(get_training_corpus(), 8_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ptb_filtered_lowcase_bpe_tokenizer_8k/tokenizer_config.json',\n",
       " 'ptb_filtered_lowcase_bpe_tokenizer_8k/special_tokens_map.json',\n",
       " 'ptb_filtered_lowcase_bpe_tokenizer_8k/vocab.json',\n",
       " 'ptb_filtered_lowcase_bpe_tokenizer_8k/merges.txt',\n",
       " 'ptb_filtered_lowcase_bpe_tokenizer_8k/added_tokens.json',\n",
       " 'ptb_filtered_lowcase_bpe_tokenizer_8k/tokenizer.json')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer.save_pretrained(\"ptb_filtered_lowcase_bpe_tokenizer_8k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/omar/miniconda3/envs/structformer/lib/python3.6/site-packages/huggingface_hub/hf_api.py:1004: FutureWarning: `create_repo` now takes `token` as an optional positional argument. Be sure to adapt your code!\n",
      "  FutureWarning,\n",
      "Cloning https://huggingface.co/omarmomen/ptb_filtered_lowcase_bpe_tokenizer_8 into local empty directory.\n",
      "To https://huggingface.co/omarmomen/ptb_filtered_lowcase_bpe_tokenizer_8\n",
      "   8427847..7d165c7  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/omarmomen/ptb_filtered_lowcase_bpe_tokenizer_8/commit/7d165c7f84f6f1b95f0c4a959b2668f2d09a700a'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer.push_to_hub(\"ptb_filtered_lowcase_bpe_tokenizer_8\", use_temp_dir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptb_corpus = data_ptb_subword.SubWord_Corpus(\"omarmomen/ptb_filtered_lowcase_bpe_tokenizer_8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_POSS = ['n\\'t', '\\'s', '\\'m', '\\'re', '\\'ve', '\\'d', '\\'ll', '%']\n",
    "\n",
    "def filter_words(tree):\n",
    "    words = []\n",
    "    for w, tag in tree.pos():\n",
    "        if tag in WORD_TAGS:\n",
    "            w = w.lower()\n",
    "            words.append(w)\n",
    "    return words\n",
    "\n",
    "def tree2list(tree):\n",
    "    if isinstance(tree, nltk.Tree):\n",
    "        if (tree.label() in WORD_TAGS):\n",
    "            w = tree.leaves()[0]\n",
    "            return w\n",
    "        else:\n",
    "            root = []\n",
    "            for child in tree:\n",
    "                c = tree2list(child)\n",
    "                if c:\n",
    "                    root.append(c)\n",
    "            if len(root) > 1:\n",
    "                return root\n",
    "            elif len(root) == 1:\n",
    "                return root[0]\n",
    "    return []\n",
    "\n",
    "def merge_neg_poss(tree, prev_root=None):\n",
    "    if isinstance(tree, list):\n",
    "        root = []\n",
    "        for child in tree:\n",
    "            if isinstance(child, str) and (child.lower() in NEG_POSS):\n",
    "                if len(root) > 0: \n",
    "                    if len(root) > 1 and isinstance(root[-1], list) and isinstance(root[-2], str): # root = [First, [of, america] ] child = 's \n",
    "                        first_str = root[-2]\n",
    "                        while not isinstance(root[-1], str):\n",
    "                            root = root[-1]\n",
    "                        root[-1] = root[-1] + child\n",
    "                        temp1 = [first_str]\n",
    "                        temp2 = root\n",
    "                        temp1.append(temp2)\n",
    "                        root = temp1\n",
    "                    else:\n",
    "                        while not isinstance(root[-1], str):\n",
    "                            root = root[-1]\n",
    "                        root[-1] = root[-1] + child\n",
    "                else:\n",
    "                    try:\n",
    "                        while not isinstance(prev_root[-1], str):\n",
    "                            prev_root = prev_root[-1]\n",
    "                        prev_root[-1] = prev_root[-1] + child\n",
    "                    except IndexError:\n",
    "                        pass\n",
    "            else:\n",
    "                root.append(merge_neg_poss(child,root))\n",
    "        return root\n",
    "    return tree\n",
    "\n",
    "def merge_strings_in_tree(tree, neg_poss):\n",
    "    def merge_with_last_string(node, string_to_merge):\n",
    "        if isinstance(node[-1], str):\n",
    "            node[-1] += string_to_merge\n",
    "        elif isinstance(node[-1], list):\n",
    "            merge_with_last_string(node[-1], string_to_merge)\n",
    "\n",
    "    def merge_recursive(node, prev_node=None):\n",
    "        if isinstance(node, list):\n",
    "            new_node = []\n",
    "            for item in node:\n",
    "                merged_item = merge_recursive(item, new_node if new_node else prev_node)\n",
    "                if merged_item is not None:\n",
    "                    new_node.append(merged_item)\n",
    "            return new_node\n",
    "        else:\n",
    "            if node in neg_poss and prev_node is not None:\n",
    "                merge_with_last_string(prev_node, node)\n",
    "                return None\n",
    "            else:\n",
    "                return [node]\n",
    "\n",
    "    def remove_wrapping_lists(node):\n",
    "        if isinstance(node, list):\n",
    "            if len(node) == 1 and isinstance(node[0], str):\n",
    "                return node[0]\n",
    "            else:\n",
    "                return [remove_wrapping_lists(item) for item in node]\n",
    "        return node\n",
    "\n",
    "    merged_tree = merge_recursive(tree)\n",
    "    return remove_wrapping_lists(merged_tree)\n",
    "\n",
    "def convert_tree(tree, tokenizer):\n",
    "    tokenized_tree = []\n",
    "    for word in tree:\n",
    "        if isinstance(word, str):\n",
    "            subword_tokens = tokenizer.tokenize(word)\n",
    "            if len(subword_tokens) > 1:\n",
    "                tokenized_subwords = convert_tree(subword_tokens, tokenizer)\n",
    "                tokenized_tree.extend([tokenized_subwords])\n",
    "            else:\n",
    "                tokenized_tree.append(word)\n",
    "        else:\n",
    "            tokenized_subtree = convert_tree(word, tokenizer)\n",
    "            tokenized_tree.append(tokenized_subtree)\n",
    "    return tokenized_tree\n",
    "\n",
    "def tokenize_tree(tree, tokenizer, flag):\n",
    "    if isinstance(tree, list):\n",
    "        tokenized_tree = []\n",
    "        for child in tree:\n",
    "            if isinstance(child, str):\n",
    "                if flag:\n",
    "                    subwords = tokenizer.tokenize(child.lower())\n",
    "                else:\n",
    "                    subwords = tokenizer.tokenize(' '+child.lower())\n",
    "                flag = False\n",
    "                subwords = subwords[0] if len(subwords) == 1 else subwords\n",
    "                tokenized_tree.append(subwords)\n",
    "            else:\n",
    "                t, flag = tokenize_tree(child, tokenizer, flag) \n",
    "                tokenized_tree.append(t)\n",
    "        return tokenized_tree, flag\n",
    "    return tree, flag\n",
    "\n",
    "def count_leaves(tree):\n",
    "    if isinstance(tree, list):\n",
    "        count = 0\n",
    "        for child in tree:\n",
    "            count += count_leaves(child)\n",
    "        return count\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def double_check_detokenize(sen):\n",
    "    words = sen.split(' ')\n",
    "    for i in range(1, len(words)):\n",
    "        if words[i] in NEG_POSS:\n",
    "            words[i-1] = words[i-1] + words[i]\n",
    "            words[i] = ''\n",
    "    sen = ' '.join(words)\n",
    "    \n",
    "    words = sen.strip().split(' ')\n",
    "    for i in range(len(words)):\n",
    "        if words[i].lower() == 'cannot':\n",
    "            words[i] = 'can'\n",
    "            words.insert(i+1, 'not')\n",
    "        elif words[i].lower() == 'gonna':\n",
    "            words[i] = 'gon'\n",
    "            words.insert(i+1, 'na')\n",
    "    sen = ' '.join(words)\n",
    "    \n",
    "    return sen.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_tree = ptb_corpus.test_nltktrees[412]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"superconcentrates aren't entirely new for p&g\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen  = detok.detokenize(filter_words(sen_tree))\n",
    "sen = double_check_detokenize(sen)\n",
    "sen\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ptb_corpus.tokenizer.encode(sen, add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['su',\n",
       " 'per',\n",
       " 'con',\n",
       " 'cent',\n",
       " 'rates',\n",
       " 'Ġaren',\n",
       " \"'t\",\n",
       " 'Ġentirely',\n",
       " 'Ġnew',\n",
       " 'Ġfor',\n",
       " 'Ġp',\n",
       " '&',\n",
       " 'g']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ptb_corpus.tokenizer.tokenize(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['su', 'per', 'con', 'cent', 'rates'],\n",
       "  [['Ġaren', \"'t\"], ['Ġentirely', 'Ġnew'], ['Ġfor', ['Ġp', '&', 'g']]]],\n",
       " 13)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_tree = tree2list(sen_tree) # get a constituent tree in list form (conatining only POS words)\n",
    "if isinstance(list_tree, str):\n",
    "    list_tree = [list_tree]\n",
    "detokenized_tree = merge_strings_in_tree(list_tree, NEG_POSS) # merge negation and possesion and special tokens \n",
    "tokenized_tree, _ = tokenize_tree(detokenized_tree, ptb_corpus.tokenizer, flag=True) # subword tokenize the tree\n",
    "tokenized_tree, count_leaves(tokenized_tree) # count the number of subwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Ġaren', \"'t\"], ['Ġentirely', 'Ġnew'], ['Ġfor', ['Ġp', '&', 'g']]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_tree[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Superconcentrates', ['are', \"n't\", ['entirely', 'new'], ['for', 'P&G']]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'study'],\n",
       " ['shows',\n",
       "  [['that',\n",
       "    [[[['nearly', '40%']], ['of', ['the', 'homeless', 'population']]],\n",
       "     ['is', ['made', 'up', ['of', ['women', 'and', 'children']]]]]],\n",
       "   'and',\n",
       "   ['that',\n",
       "    [[[['only', '25%']], ['of', ['the', 'homeless']]],\n",
       "     ['exhibits',\n",
       "      [['some', 'combination'],\n",
       "       ['of', [['drug', 'alcohol', 'and', 'mental'], 'problems']]]]]]]]]"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detokenized_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_checkpoint = torch.load('../trained_models/subword_structformer_in_parser_1111_44.pt')\n",
    "loaded_model = loaded_checkpoint[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = ptb_corpus.test_sens[412]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = ptb_corpus.tokenizer.tokenize(sen)\n",
    "data = torch.LongTensor([ptb_corpus.test[412]]).to(device)\n",
    "pos = torch.LongTensor([list(range(len(ptb_corpus.test[412])))]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, p_dict = loaded_model(data, pos)\n",
    "block = p_dict['block']\n",
    "cibling = p_dict['cibling']\n",
    "head = p_dict['head']\n",
    "distance = p_dict['distance']\n",
    "height = p_dict['height']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "distance = distance.clone().squeeze(0).cpu().detach().numpy().tolist()\n",
    "height = height.clone().squeeze(0).cpu().detach().numpy().tolist()\n",
    "head = head.clone().squeeze(0).cpu().detach().numpy()\n",
    "max_height = np.max(height)\n",
    "\n",
    "parse_tree = tree_utils.build_tree(distance, sen)\n",
    "\n",
    "model_out, _ = tree_utils.get_brackets(parse_tree)\n",
    "std_out, _ = tree_utils.get_brackets(sen_tree)\n",
    "overlap = model_out.intersection(std_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"'t\", 'Ġentirely'], 'Ġnew']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_tree[1][0][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_file_path = '../data/ptb/test.conllu'\n",
    "\n",
    "with open(trg_file_path, 'r') as trg_file:\n",
    "    trg_string = trg_file.read().strip()\n",
    "    trg_string_list = trg_string.split('\\n\\n')\n",
    "    try:\n",
    "      trg_list = [\n",
    "          DependencyGraph(t, top_relation_label='root') for t in trg_string_list\n",
    "      ]\n",
    "    except ValueError:\n",
    "\n",
    "      def extract_10_cells(cells, index):\n",
    "        line_index, word, lemma, tag, _, head, rel, _, _, _ = cells\n",
    "        try:\n",
    "          index = int(line_index)\n",
    "        except ValueError:\n",
    "          # index can't be parsed as an integer, use default\n",
    "          pass\n",
    "        return index, word, lemma, tag, tag, '', head, rel\n",
    "\n",
    "      trg_list = [\n",
    "          DependencyGraph(\n",
    "              t, top_relation_label='root', cell_extractor=extract_10_cells)\n",
    "          for t in trg_string_list\n",
    "      ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_file_path = '../data/ptb/test.conllu'\n",
    "\n",
    "with open(trg_file_path, 'r') as trg_file:\n",
    "    trg_string = trg_file.read().strip()\n",
    "    trg_string_list = trg_string.split('\\n\\n')\n",
    "    try:\n",
    "      trg_list = [\n",
    "          DependencyGraph(t, top_relation_label='root') for t in trg_string_list\n",
    "      ]\n",
    "    except ValueError:\n",
    "\n",
    "      def extract_10_cells(cells, index):\n",
    "        line_index, word, lemma, tag, _, head, rel, _, _, _ = cells\n",
    "        try:\n",
    "          index = int(line_index)\n",
    "        except ValueError:\n",
    "          # index can't be parsed as an integer, use default\n",
    "          pass\n",
    "        return index, word, lemma, tag, tag, '', head, rel\n",
    "\n",
    "      trg_list = [\n",
    "          DependencyGraph(\n",
    "              t, top_relation_label='root', cell_extractor=extract_10_cells)\n",
    "          for t in trg_string_list\n",
    "      ]\n",
    "\n",
    "\n",
    "all_indices = []\n",
    "all_deps = []\n",
    "c = 0\n",
    "for j in range(len(trg_list)):\n",
    "    included_indices = []\n",
    "    deps = []\n",
    "    nodes_addresses = []\n",
    "    for i in range(len(trg_list[j].nodes)):\n",
    "        if trg_list[j].nodes[i]['ctag'] in WORD_TAGS:\n",
    "            node_address = trg_list[j].nodes[i]['address']\n",
    "            node_word = trg_list[j].nodes[i]['word']\n",
    "            nodes_addresses.append(node_address)\n",
    "            included_indices.append(dict({node_address:node_word}))\n",
    "    \n",
    "    removed_nodes_parents = dict()\n",
    "    for i in range(len(trg_list[j].nodes)):\n",
    "        node_address = trg_list[j].nodes[i]['address']\n",
    "        if node_address not in nodes_addresses and node_address != 0:\n",
    "            head = trg_list[j].nodes[i]['head']\n",
    "            if head in nodes_addresses:\n",
    "                removed_nodes_parents[node_address] = head\n",
    "            else:\n",
    "                head_2 = trg_list[j].get_by_address(head)['head']\n",
    "                if head_2 in nodes_addresses:\n",
    "                    removed_nodes_parents[node_address] = head_2\n",
    "                else:\n",
    "                    c = c + 1\n",
    "                    #print(f\"MOSSEBA head1 {head} head2 {head_2} node_address {node_address} in {j}\")\n",
    "                    #print(trg_list[j].to_conll(10))\n",
    "                    #raise AssertionError\n",
    "    \n",
    "    # for i in range(len(trg_list[j].nodes)):\n",
    "    #     node_address = trg_list[j].nodes[i]['address']\n",
    "    #     if node_address in nodes_addresses:\n",
    "    #         head = trg_list[j].nodes[i]['head']\n",
    "    #         if head in nodes_addresses:\n",
    "    #             deps.append([node_address, head])\n",
    "    #         elif head == 0:\n",
    "    #             deps.append([node_address, head])\n",
    "    #         else: # head is not in nodes_addresses, parent of node got filtered out\n",
    "    #             deps.append([node_address, removed_nodes_parents[head]])\n",
    "    #             #raise AssertionError(f\"in {j}, head {head} is not in nodes_addresses {nodes_addresses} in {trg_list[j].nodes}\")\n",
    "            \n",
    "    \n",
    "    \n",
    "    # set_all_indices = set()\n",
    "    # for item in included_indices:\n",
    "    #     set_all_indices.add(list(item.keys())[0])\n",
    "    # set_all_indices.add(0)\n",
    "    # set_all_deps = set()\n",
    "    # for item in deps:\n",
    "    #     set_all_deps.add(item[0])\n",
    "    #     set_all_deps.add(item[1])\n",
    "    \n",
    "    # if set_all_indices != set_all_deps:\n",
    "    #     print(included_indices)\n",
    "    #     print(deps)\n",
    "    #     print(set_all_indices)\n",
    "    #     print(set_all_deps)\n",
    "    #     print()\n",
    "    #     raise AssertionError(f\"at 1 seneence {j}\")\n",
    "    \n",
    "    # all_indices.append(included_indices)\n",
    "    # all_deps.append(deps)\n",
    "\n",
    "# k = 0\n",
    "# for indices, deps in zip(all_indices, all_deps):\n",
    "#     assert len(all_indices[k]) == len(all_deps[k]), f\"at 1: length of indices and deps are not equal for sentence {k}\"\n",
    "#     l1 = len(all_indices[k])\n",
    "#     m1 = copy.deepcopy(all_indices[k])\n",
    "#     m2 = copy.deepcopy(all_deps[k])\n",
    "#     removed_indices = []\n",
    "#     removed_deps = []\n",
    "#     flag = False\n",
    "#     for i, ind in enumerate(indices):\n",
    "#         token = list(ind.values())[0] # get the token\n",
    "#         if token.lower() in NEG_POSS:\n",
    "#             index = list(ind.keys())[0]\n",
    "#             # merge the token with the previous token\n",
    "#             all_indices[k][i-1][list(indices[i-1].keys())[0]] += token\n",
    "#             # remove the token from the indices\n",
    "#             removed_indices.append(ind)\n",
    "            \n",
    "#             # get the head of token\n",
    "#             head = None\n",
    "#             for j, dep in enumerate(deps):\n",
    "#                 if dep[0] == index:\n",
    "#                     head = dep[1]\n",
    "#                     break\n",
    "            \n",
    "#             # update the dependencies\n",
    "#             for j, dep in enumerate(deps):\n",
    "#                 if dep[0] == index:\n",
    "#                     removed_deps.append(dep)\n",
    "#                 if dep[1] == index:\n",
    "#                     flag = True\n",
    "#                     if all_deps[k][j][0] != list(all_indices[k][i-1].keys())[0]:\n",
    "#                         all_deps[k][j][1] = list(all_indices[k][i-1].keys())[0]\n",
    "#                     else: # if the head of the token is the token itself\n",
    "#                         all_deps[k][j][1] = head\n",
    "            \n",
    "#     for item in removed_indices:\n",
    "#         all_indices[k].remove(item)\n",
    "#     for item in removed_deps:\n",
    "#         all_deps[k].remove(item)            \n",
    "#     try:\n",
    "#         assert len(all_indices[k]) == len(all_deps[k]), f\"at 2: length of indices and deps are not equal for sentence {k}\"\n",
    "#     except AssertionError as e:\n",
    "#         print(k)\n",
    "#         print(all_indices[k])\n",
    "#         print(all_deps[k])\n",
    "#         print(len(all_indices[k]), len(all_deps[k]))\n",
    "#         print()\n",
    "#         raise AssertionError(f\"{e} at sentence {k}\")\n",
    "#     l2 = len(all_indices[k])\n",
    "    \n",
    "#     set_all_indices = set()\n",
    "#     for item in all_indices[k]:\n",
    "#         set_all_indices.add(list(item.keys())[0])\n",
    "#     set_all_indices.add(0)\n",
    "#     set_all_deps = set()\n",
    "#     for item in all_deps[k]:\n",
    "#         set_all_deps.add(item[0])\n",
    "#         set_all_deps.add(item[1])\n",
    "    \n",
    "#     if set_all_indices != set_all_deps:\n",
    "#         print(k)\n",
    "#         print(m1)\n",
    "#         print(all_indices[k])\n",
    "#         print(m2)\n",
    "#         print(all_deps[k])\n",
    "#         print(set_all_indices)\n",
    "#         print(set_all_deps)\n",
    "#         print()\n",
    "#         raise AssertionError(f\"at sentence {k}\")\n",
    "    \n",
    "#     if False:\n",
    "#         print(k)\n",
    "#         print(m1)\n",
    "#         print(all_indices[k])\n",
    "#         print(m2)\n",
    "#         print(all_deps[k])\n",
    "#         print(l1, l2)\n",
    "#         print(\"=====================================\")\n",
    "#     k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tA\ta\tDT\tDT\t_\t2\tdet\t_\t_\n",
      "2\tspokesman\tspokesman\tNN\tNN\t_\t3\tnsubj\t_\t_\n",
      "3\tadded\tadd\tVBD\tVBD\t_\t0\troot\t_\t_\n",
      "4\tthat\tthat\tIN\tIN\t_\t12\tmark\t_\t_\n",
      "5\tsales\tsale\tNNS\tNNS\t_\t12\tnsubj\t_\t_\n",
      "6\tin\tin\tIN\tIN\t_\t9\tcase\t_\t_\n",
      "7\tthe\tthe\tDT\tDT\t_\t9\tdet\t_\t_\n",
      "8\tcurrent\tcurrent\tJJ\tJJ\t_\t9\tamod\t_\t_\n",
      "9\tquarter\tquarter\tNN\tNN\t_\t5\tnmod\t_\t_\n",
      "10\twill\twill\tNN\tNN\t_\t12\tdep\t_\t_\n",
      "11\tabout\tabout\tRB\tRB\t_\t12\tadvmod\t_\t_\n",
      "12\tequal\tequal\tVB\tVB\t_\t3\tccomp\t_\t_\n",
      "13\tthe\tthe\tDT\tDT\t_\t15\tdet\t_\t_\n",
      "14\tyearearlier\tyearearlier\tJJ\tJJ\t_\t15\tamod\t_\t_\n",
      "15\tquarter\tquarter\tNN\tNN\t_\t17\tnmod:poss\t_\t_\n",
      "16\t's\t's\tPOS\tPOS\t_\t15\tcase\t_\t_\n",
      "17\tfigure\tfigure\tNN\tNN\t_\t12\tdobj\t_\t_\n",
      "18\t,\t,\t,\t,\t_\t17\tpunct\t_\t_\n",
      "19\twhen\twhen\tWRB\tWRB\t_\t21\tadvmod\t_\t_\n",
      "20\tNewport\tNewport\tNNP\tNNP\t_\t21\tnsubj\t_\t_\n",
      "21\treported\treport\tVBD\tVBD\t_\t17\tacl:relcl\t_\t_\n",
      "22\tnet\tnet\tJJ\tJJ\t_\t23\tamod\t_\t_\n",
      "23\tincome\tincome\tNN\tNN\t_\t21\tdobj\t_\t_\n",
      "24\tof\tof\tIN\tIN\t_\t25\tcase\t_\t_\n",
      "25\t$\t$\t$\t$\t_\t23\tnmod\t_\t_\n",
      "26\t1.7\t1.7\tCD\tCD\t_\t27\tcompound\t_\t_\n",
      "27\tmillion\tmillion\tCD\tCD\t_\t25\tnummod\t_\t_\n",
      "28\t,\t,\t,\t,\t_\t25\tpunct\t_\t_\n",
      "29\tor\tor\tCC\tCC\t_\t25\tcc\t_\t_\n",
      "30\t21\t21\tCD\tCD\t_\t31\tnummod\t_\t_\n",
      "31\tcents\tcent\tNNS\tNNS\t_\t25\tconj\t_\t_\n",
      "32\ta\ta\tDT\tDT\t_\t33\tdet\t_\t_\n",
      "33\tshare\tshare\tNN\tNN\t_\t31\tnmod:npmod\t_\t_\n",
      "34\t,\t,\t,\t,\t_\t23\tpunct\t_\t_\n",
      "35\ton\ton\tIN\tIN\t_\t36\tcase\t_\t_\n",
      "36\t$\t$\t$\t$\t_\t23\tnmod\t_\t_\n",
      "37\t14.1\t14.1\tCD\tCD\t_\t38\tcompound\t_\t_\n",
      "38\tmillion\tmillion\tCD\tCD\t_\t36\tnummod\t_\t_\n",
      "39\tin\tin\tIN\tIN\t_\t40\tcase\t_\t_\n",
      "40\tsales\tsale\tNNS\tNNS\t_\t36\tnmod\t_\t_\n",
      "41\t.\t.\t.\t.\t_\t3\tpunct\t_\t_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(trg_list[194].to_conll(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'address': 1,\n",
       " 'word': 'Shares',\n",
       " 'lemma': 'Shares',\n",
       " 'ctag': 'NNP',\n",
       " 'tag': 'NNP',\n",
       " 'feats': '_',\n",
       " 'head': 13,\n",
       " 'deps': defaultdict(list, {'nmod': [3]}),\n",
       " 'rel': 'nsubj'}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_list[21].nodes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tShares\tShares\tNNP\tNNP\t_\t13\tnsubj\t_\t_\n",
      "2\tof\tof\tIN\tIN\t_\t3\tcase\t_\t_\n",
      "3\tUAL\tUAL\tNNP\tNNP\t_\t1\tnmod\t_\t_\n",
      "4\t,\t,\t,\t,\t_\t3\tpunct\t_\t_\n",
      "5\tthe\tthe\tDT\tDT\t_\t6\tdet\t_\t_\n",
      "6\tparent\tparent\tNN\tNN\t_\t3\tappos\t_\t_\n",
      "7\tof\tof\tIN\tIN\t_\t9\tcase\t_\t_\n",
      "8\tUnited\tUnited\tNNP\tNNP\t_\t9\tcompound\t_\t_\n",
      "9\tAirlines\tAirlines\tNNP\tNNP\t_\t6\tnmod\t_\t_\n",
      "10\t,\t,\t,\t,\t_\t3\tpunct\t_\t_\n",
      "11\twere\tbe\tVBD\tVBD\t_\t13\tcop\t_\t_\n",
      "12\textremely\textremely\tRB\tRB\t_\t13\tadvmod\t_\t_\n",
      "13\tactive\tactive\tJJ\tJJ\t_\t0\troot\t_\t_\n",
      "14\tall\tall\tDT\tDT\t_\t15\tdet\t_\t_\n",
      "15\tday\tday\tNN\tNN\t_\t13\tnmod:tmod\t_\t_\n",
      "16\tFriday\tFriday\tNNP\tNNP\t_\t13\tnmod:tmod\t_\t_\n",
      "17\t,\t,\t,\t,\t_\t13\tpunct\t_\t_\n",
      "18\treacting\treact\tVBG\tVBG\t_\t13\tadvcl\t_\t_\n",
      "19\tto\tto\tTO\tTO\t_\t20\tcase\t_\t_\n",
      "20\tnews\tnews\tNN\tNN\t_\t18\tnmod\t_\t_\n",
      "21\tand\tand\tCC\tCC\t_\t20\tcc\t_\t_\n",
      "22\trumors\trumor\tNNS\tNNS\t_\t20\tconj\t_\t_\n",
      "23\tabout\tabout\tIN\tIN\t_\t29\tcase\t_\t_\n",
      "24\tthe\tthe\tDT\tDT\t_\t29\tdet\t_\t_\n",
      "25\tproposed\tpropose\tVBN\tVBN\t_\t29\tamod\t_\t_\n",
      "26\t$\t$\t$\t$\t_\t29\tamod\t_\t_\n",
      "27\t6.79\t6.79\tCD\tCD\t_\t28\tcompound\t_\t_\n",
      "28\tbillion\tbillion\tCD\tCD\t_\t26\tnummod\t_\t_\n",
      "29\tbuy-out\tbuy-out\tNN\tNN\t_\t20\tnmod\t_\t_\n",
      "30\tof\tof\tIN\tIN\t_\t32\tcase\t_\t_\n",
      "31\tthe\tthe\tDT\tDT\t_\t32\tdet\t_\t_\n",
      "32\tairline\tairline\tNN\tNN\t_\t29\tnmod\t_\t_\n",
      "33\tby\tby\tIN\tIN\t_\t36\tcase\t_\t_\n",
      "34\tan\ta\tDT\tDT\t_\t36\tdet\t_\t_\n",
      "35\temployee-management\temployee-management\tJJ\tJJ\t_\t36\tamod\t_\t_\n",
      "36\tgroup\tgroup\tNN\tNN\t_\t29\tnmod\t_\t_\n",
      "37\t.\t.\t.\t.\t_\t13\tpunct\t_\t_\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(trg_list[21].to_conll(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "seta = set()\n",
    "setb = set()\n",
    "\n",
    "seta.add(3)\n",
    "seta.add(1)\n",
    "seta.add(2)\n",
    "seta.add(3)\n",
    "\n",
    "setb.add(1)\n",
    "setb.add(2)\n",
    "setb.add(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({1, 2, 3}, {1, 2, 3})"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seta, setb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seta == setb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_indices[0].remove({7: 'Monday'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{1: 'No'}, {3: 'it'}, {4: \"wasn't\"}, {6: 'Black'}]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "list.remove(x): x not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-7b70e71a5db0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: list.remove(x): x not in list"
     ]
    }
   ],
   "source": [
    "a.remove([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "indices_to_remove = [2, 5, 8]\n",
    "\n",
    "new_list = [item for i, item in enumerate(original_list) if i not in indices_to_remove]\n",
    "\n",
    "print(new_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model =  torch.load('../trained_models/1111_subword.pt')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list.remove(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_subword_constituents(model_out, std_out, sen):\n",
    "  true_subword_constituents = []\n",
    "  for const in std_out:\n",
    "      c = sen[const[0]:const[1]]\n",
    "      try:\n",
    "        if not c[0].startswith('Ġ'):\n",
    "            flag=True\n",
    "            for token in c:\n",
    "                if token.startswith('Ġ'):\n",
    "                    flag=False\n",
    "                    break\n",
    "            if flag:\n",
    "                print(const)\n",
    "                print(c)\n",
    "                true_subword_constituents.append(const)\n",
    "        else:\n",
    "            flag=True\n",
    "            for token in c[1:]:\n",
    "                if token.startswith('Ġ'):\n",
    "                    flag=False\n",
    "                    break\n",
    "            if flag:\n",
    "                print(const)\n",
    "                print(c)\n",
    "                true_subword_constituents.append(const)\n",
    "      except Exception as e:\n",
    "        print(\"problem sent: \", sen)\n",
    "        raise Exception(\"Error: \", e)\n",
    "  \n",
    "  total_subword_constituents = len(true_subword_constituents)\n",
    "  correct_subword_constituents = 0\n",
    "  \n",
    "  print(\"true_subword_constituents: \", true_subword_constituents)\n",
    "  print(\"model_out: \", model_out)\n",
    "  \n",
    "  for cons in true_subword_constituents:\n",
    "      if cons in model_out:\n",
    "        correct_subword_constituents += 1\n",
    "      else:\n",
    "        print(\"uncaught_constituents: \", sen[cons[0]:cons[1]])\n",
    "        \n",
    "  if total_subword_constituents == 0:\n",
    "    return 1.0\n",
    "  else:\n",
    "    return correct_subword_constituents/total_subword_constituents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 24)\n",
      "['Ġisn', \"'t\"]\n",
      "(13, 15)\n",
      "['Ġinc', '.']\n",
      "(3, 6)\n",
      "['Ġmag', 'u', 'ire']\n",
      "(0, 2)\n",
      "['count', 'ered']\n",
      "(9, 12)\n",
      "['Ġhen', 'd', 'erson']\n",
      "true_subword_constituents:  [(22, 24), (13, 15), (3, 6), (0, 2), (9, 12)]\n",
      "model_out:  {(20, 27), (15, 27), (2, 6), (17, 19), (20, 22), (22, 24), (13, 15), (24, 26), (16, 19), (3, 5), (2, 7), (9, 13), (7, 15), (9, 15), (22, 27), (24, 27), (1, 27), (19, 27), (16, 27), (9, 11), (9, 12), (3, 6), (2, 15), (2, 27), (8, 15)}\n",
      "uncaught_constituents:  ['count', 'ered']\n",
      "f1: 0.6818, prec: 0.6000, reca: 0.7895, subword_reca: 0.8000\n",
      "=========================================\n",
      "(6, 9)\n",
      "['Ġfree', '-', 'fall']\n",
      "(12, 14)\n",
      "['Ġcan', \"'t\"]\n",
      "true_subword_constituents:  [(6, 9), (12, 14)]\n",
      "model_out:  {(4, 10), (5, 9), (1, 3), (6, 9), (3, 12), (4, 9), (6, 8), (12, 15), (1, 16), (3, 10), (13, 15), (10, 12), (12, 16), (3, 16)}\n",
      "uncaught_constituents:  ['Ġcan', \"'t\"]\n",
      "f1: 0.4000, prec: 0.3571, reca: 0.4545, subword_reca: 0.5000\n",
      "=========================================\n",
      "(0, 3)\n",
      "['spec', 'ul', 'ators']\n",
      "true_subword_constituents:  [(0, 3)]\n",
      "model_out:  {(4, 10), (5, 9), (1, 3), (6, 9), (5, 10), (6, 8), (10, 17), (14, 17), (11, 17), (3, 10), (13, 17), (12, 17), (0, 3), (3, 17), (15, 17)}\n",
      "f1: 0.5385, prec: 0.4667, reca: 0.6364, subword_reca: 1.0000\n",
      "=========================================\n",
      "(16, 18)\n",
      "['Ġwar', 'm']\n",
      "(18, 20)\n",
      "['Ġaut', 'umn']\n",
      "true_subword_constituents:  [(16, 18), (18, 20)]\n",
      "model_out:  {(1, 21), (6, 9), (1, 3), (6, 11), (15, 20), (21, 28), (3, 11), (26, 28), (11, 14), (3, 14), (7, 9), (25, 28), (21, 25), (22, 25), (4, 6), (15, 21), (1, 14), (16, 18), (9, 11), (11, 13), (1, 28), (3, 6), (14, 21), (15, 18), (18, 20), (23, 25)}\n",
      "f1: 0.5238, prec: 0.4231, reca: 0.6875, subword_reca: 1.0000\n",
      "=========================================\n",
      "(37, 41)\n",
      "['Ġ6', '.', '9', '%']\n",
      "(46, 51)\n",
      "['Ġg', 'arg', 'ant', 'u', 'an']\n",
      "(3, 5)\n",
      "['Ġlight', 'ning']\n",
      "(31, 36)\n",
      "['Ġ190', '.', '58', '-', 'point']\n",
      "(26, 29)\n",
      "['Ġch', 'al', 'king']\n",
      "(14, 18)\n",
      "['Ġsur', 'ren', 'd', 'ered']\n",
      "true_subword_constituents:  [(37, 41), (46, 51), (3, 5), (31, 36), (26, 29), (14, 18)]\n",
      "model_out:  {(24, 30), (14, 17), (36, 42), (46, 51), (26, 28), (24, 26), (42, 53), (1, 6), (11, 14), (19, 22), (2, 5), (24, 42), (45, 53), (47, 49), (38, 40), (31, 36), (26, 29), (6, 10), (14, 18), (10, 18), (30, 42), (7, 10), (11, 18), (18, 53), (45, 52), (32, 34), (19, 21), (43, 45), (37, 41), (46, 49), (2, 6), (6, 18), (12, 14), (22, 24), (18, 24), (42, 45), (19, 24), (45, 51), (32, 35), (24, 53), (37, 40), (3, 5), (7, 9), (1, 53), (15, 17), (24, 29), (36, 41), (46, 50), (6, 53), (32, 36), (30, 36)}\n",
      "f1: 0.4051, prec: 0.3137, reca: 0.5714, subword_reca: 1.0000\n",
      "=========================================\n",
      "(9, 13)\n",
      "['Ġ10', '8', '.', '1']\n",
      "(0, 4)\n",
      "['fin', 'al', '-', 'hour']\n",
      "(5, 8)\n",
      "['Ġaccel', 'er', 'ated']\n",
      "true_subword_constituents:  [(9, 13), (0, 4), (5, 8)]\n",
      "model_out:  {(1, 21), (18, 20), (9, 14), (4, 8), (10, 13), (8, 17), (18, 21), (5, 8), (2, 17), (15, 17), (8, 14), (9, 13), (2, 21), (5, 7), (4, 17), (17, 21), (2, 4), (8, 15), (11, 13)}\n",
      "uncaught_constituents:  ['fin', 'al', '-', 'hour']\n",
      "f1: 0.3750, prec: 0.3158, reca: 0.4615, subword_reca: 0.6667\n",
      "=========================================\n",
      "(6, 10)\n",
      "['Ġ25', '1', '.', '2']\n",
      "true_subword_constituents:  [(6, 10)]\n",
      "model_out:  {(2, 6), (4, 6), (12, 14), (6, 11), (8, 11), (1, 14), (8, 10), (6, 12), (2, 12), (3, 6), (7, 11), (2, 14)}\n",
      "uncaught_constituents:  ['Ġ25', '1', '.', '2']\n",
      "f1: 0.4762, prec: 0.4167, reca: 0.5556, subword_reca: 0.0000\n",
      "=========================================\n",
      "(6, 11)\n",
      "['Ġ25', '6', '9', '.', '26']\n",
      "true_subword_constituents:  [(6, 11)]\n",
      "model_out:  {(1, 3), (6, 8), (5, 11), (6, 11), (6, 10), (8, 10), (1, 4), (1, 11), (4, 11)}\n",
      "f1: 0.4615, prec: 0.3333, reca: 0.7500, subword_reca: 1.0000\n",
      "=========================================\n",
      "(1, 3)\n",
      "['Ġdow', \"'s\"]\n",
      "(21, 23)\n",
      "['Ġoct', '.']\n",
      "(12, 16)\n",
      "['Ġ50', '8', '-', 'point']\n",
      "true_subword_constituents:  [(1, 3), (21, 23), (12, 16)]\n",
      "model_out:  {(11, 16), (5, 9), (6, 9), (20, 25), (21, 23), (10, 19), (1, 4), (1, 19), (12, 16), (16, 19), (11, 19), (4, 9), (1, 25), (6, 8), (4, 19), (14, 16), (20, 24), (13, 16), (16, 18), (19, 25), (9, 19), (2, 4), (21, 24)}\n",
      "uncaught_constituents:  ['Ġdow', \"'s\"]\n",
      "f1: 0.3158, prec: 0.2609, reca: 0.4000, subword_reca: 0.6667\n",
      "=========================================\n",
      "(11, 16)\n",
      "['Ġ12', 'th', '-', 'wor', 'st']\n",
      "(25, 29)\n",
      "['Ġ15', '6', '.', '83']\n",
      "(30, 32)\n",
      "['Ġ8', '%']\n",
      "(7, 9)\n",
      "['Ġd', 'ive']\n",
      "(5, 7)\n",
      "['Ġdow', \"'s\"]\n",
      "(19, 21)\n",
      "['Ġsharp', 'est']\n",
      "true_subword_constituents:  [(11, 16), (25, 29), (30, 32), (7, 9), (5, 7), (19, 21)]\n",
      "model_out:  {(11, 16), (5, 9), (6, 9), (1, 3), (30, 32), (10, 17), (26, 28), (3, 21), (1, 37), (11, 14), (35, 37), (4, 9), (9, 21), (26, 29), (21, 34), (29, 34), (9, 17), (24, 34), (32, 34), (3, 37), (19, 21), (25, 29), (11, 13), (22, 24), (3, 9), (21, 37), (25, 34), (18, 21), (34, 37), (7, 9), (14, 16), (10, 16), (29, 32), (17, 21), (21, 24)}\n",
      "uncaught_constituents:  ['Ġdow', \"'s\"]\n",
      "f1: 0.4746, prec: 0.4000, reca: 0.5833, subword_reca: 0.8333\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(10, 20):\n",
    "    sen = ptb_corpus.tokenizer.tokenize(ptb_corpus.test_sens[i])\n",
    "    x = ptb_corpus.test[i]\n",
    "\n",
    "    data = torch.LongTensor([x]).to(device)\n",
    "    pos = torch.LongTensor([list(range(len(x)))]).to(device)\n",
    "\n",
    "    _, p_dict = loaded_model(data, pos)\n",
    "    block = p_dict['block']\n",
    "    cibling = p_dict['cibling']\n",
    "    head = p_dict['head']\n",
    "    distance = p_dict['distance']\n",
    "    height = p_dict['height']\n",
    "\n",
    "    distance = distance.detach().clone().squeeze(0).cpu().numpy().tolist()\n",
    "    height = height.detach().clone().squeeze(0).cpu().numpy().tolist()\n",
    "    head = head.detach().clone().squeeze(0).cpu().numpy()\n",
    "    max_height = np.max(height)\n",
    "\n",
    "    parse_tree = tree_utils.build_tree(distance, sen)\n",
    "\n",
    "    model_out, _ = tree_utils.get_brackets(parse_tree)\n",
    "    std_out, _ = tree_utils.get_brackets(ptb_corpus.test_trees[i])\n",
    "    overlap = model_out.intersection(std_out)\n",
    "\n",
    "    prec = float(len(overlap)) / (len(model_out) + 1e-8)\n",
    "    reca = float(len(overlap)) / (len(std_out) + 1e-8)\n",
    "    if not std_out:\n",
    "        reca = 1.\n",
    "        if not model_out:\n",
    "            prec = 1.\n",
    "    f1 = 2 * prec * reca / (prec + reca + 1e-8)\n",
    "    subword_reca = recall_subword_constituents(model_out, std_out, sen)\n",
    "    print(f\"f1: {f1:.4f}, prec: {prec:.4f}, reca: {reca:.4f}, subword_reca: {subword_reca:.4f}\")\n",
    "    print(\"=========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.49999999875, 0.6666666644444444, 0.5714285648979592, 1.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec, reca, f1, subword_reca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "structformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
